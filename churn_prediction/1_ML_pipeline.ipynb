{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Import the dataset as a CSV\n",
    "df = pd.read_csv('0_churn_dataset.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show summary statistics for all columns\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for null values\n",
    "df.isnull().sum()\n",
    "# no null values, so no need to drop any rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for duplicate rows\n",
    "duplicates = df.duplicated().sum()\n",
    "print(f\"Number of Duplicate Rows:{duplicates}\")\n",
    "# no duplicate rows, so no need to drop any rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check variables and associated data type\n",
    "df.dtypes\n",
    "# Looks fine as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insight 1: Finding out proportion of customers who have churned in total\n",
    "total_churned = df['Churn'].sum()\n",
    "total_customers = df.shape[0]\n",
    "proportion = (total_churned / total_customers) * 100\n",
    "print(f\"Insight 1: {proportion}% of customers have churned in total\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means the dataset is unbalanced and we need to be careful building the model so it can predict both churners and non-churners well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insight 2: Finding out if churned customers have higher customer service calls than non-churned customers\n",
    "non_churned = df[df['Churn'] == False]\n",
    "churned = df[df['Churn'] == True]\n",
    "\n",
    "avg_service_calls_churned = churned['Customer service calls'].mean()\n",
    "avg_service_calls_non_churned = non_churned['Customer service calls'].mean()\n",
    "\n",
    "print(f\"Average customer service calls for churned customers: {avg_service_calls_churned}\")\n",
    "print(f\"Average customer service calls for non-churned customers: {avg_service_calls_non_churned}\")\n",
    "\n",
    "if avg_service_calls_churned > avg_service_calls_non_churned:\n",
    "    print(\"Insight 2: Churned customers have higher average nr. of customer service calls than non-churned customers.\")\n",
    "else:\n",
    "    print(\"Insight 2: Churned customers do not have higher average nr. of customer service calls than non-churned customers.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Insight 3: Find out if customers who were charged more than the average monthly charge are more likely to churn\n",
    "# Adding column that sums up total day charge, total eve charge, total night charge and total intl charge\n",
    "insight_3_df = df.copy()\n",
    "insight_3_df['Total charge'] = insight_3_df['Total day charge'] + insight_3_df['Total eve charge'] + insight_3_df['Total night charge'] + insight_3_df['Total intl charge']\n",
    "insight_3_df[\"Total charge\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating likelihood of churn for customers who were charged more than the average monthly charge\n",
    "# Calculate average charge\n",
    "avg_charge = insight_3_df['Total charge'].mean()\n",
    "# Extracting churned customers who were charged more than the average monthly charge\n",
    "churned_above_avg_charge = insight_3_df[insight_3_df['Total charge'] > avg_charge]\n",
    "churned_above_avg_charge.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate churn rate for customers who were charged more than the average monthly charge\n",
    "churn_rate_above_avg_charge = churned_above_avg_charge['Churn'].mean() * 100\n",
    "print(f\"The churn rate for customers who were charged more than the average monthly charge is {churn_rate_above_avg_charge}%\")\n",
    "#Calculate average churn rate for all customers\n",
    "avg_churn_rate = insight_3_df['Churn'].mean() * 100\n",
    "print(f\"The average churn rate for all customers is {avg_churn_rate}%\")\n",
    "print(f\"Insight 3: Customers who were charged more than the average monthly charge are more likely to churn than the average customer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End-to-end machine learning pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a classification problem because the target variable/label \"Churn\" is a binary variable that takes on one of two possible values, namely \"True\" or \"False\". Following, two different machine learning models will be trained to achieve as high of a F-2 score as possible.\n",
    "\n",
    "The F-2 score is chosen because it merges recall and precision into a single score, providing a good blend between how well the model can predict churners well and if it overpredicts non-churners to be churners. Unlike the F-1 score, the F-2 score puts more emphasis on recall than on precision. This makes sense because the telcom company should be more concerned about correctly predicting all possible churning customers (recall) than predicting too many non-churning customers to be churners (precision) because a churned customer likely costs the company a lot more than being overcareful with a non-churning customer. On the other hand, just focusing entirely on recall will incentivize the model just to predict all customers as churners which is not ideal because it will involve a lot of unnecessary effort on the part of the telcom company to keep customers happy that are not planning on leaving in the first place. Therefore, the F-2 score provides a very good balance and makes most sense to me as a performance metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 1: Logistic Regression\n",
    "\n",
    "This model is chosen because Logistic regression is a simple classification algorithm that can predict the probability of a binary response belonging to one class or the other. This is exactly the kind of prediction problem we have for our dataset where we predict if a customer churns or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 1: Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 1: Logistic Regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Feature Selection\n",
    "# Drop state column as it is a categorical variable with too many options to be included in the model\n",
    "model1_df = df.drop('State', axis=1)\n",
    "# Make categorical columns International Plan and Voice Mail Plan into binary columns with 1 for Yes and 0 for No, do the same for Churn\n",
    "model1_df['International plan'] = model1_df['International plan'].map({'Yes': 1, 'No': 0})\n",
    "model1_df['Voice mail plan'] = model1_df['Voice mail plan'].map({'Yes': 1, 'No': 0})\n",
    "model1_df['Churn'] = model1_df['Churn'].astype(int)\n",
    "model1_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the correlation matrix to see which variables are correlated with Churn and should be included in the model\n",
    "plt.figure(figsize=(16, 12))\n",
    "sns.heatmap(model1_df.corr(), annot=True, cmap='coolwarm')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As total eve calls is perfectly correlated with total eve charge and total eve minutes, we will drop total eve calls and total eve minutes to avoid multicollinearity\n",
    "model1_df = model1_df.drop(['Total eve calls', 'Total eve minutes'], axis=1)\n",
    "# Do the same for total day calls and total day minutes, total night calls and total night minutes as well as total intl calls and total intl minutes\n",
    "model1_df = model1_df.drop(['Total day calls', 'Total day minutes', 'Total night calls', 'Total night minutes', 'Total intl calls', 'Total intl minutes'], axis=1)\n",
    "# Also drop area code column as it barely correlates with churn and should also be categorical, not numerical\n",
    "model1_df = model1_df.drop('Area code', axis=1)\n",
    "model1_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As Number vmail messages is very highly correlated with Voice mail plan_Yes and Number vmail messages contains\n",
    "# more information, we will drop Voice mail plan_Yes to avoid multicollinearity\n",
    "model1_df = model1_df.drop(['Voice mail plan'], axis=1)\n",
    "model1_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a descriptive table with min, median, mean, max values for the numerical columns\n",
    "model1_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2: Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into training and testing sets (80/20 split)\n",
    "X = model1_df.drop(['Churn'], axis=1)  # Features\n",
    "y = model1_df['Churn']  # Target variable\n",
    "\n",
    "# Using stratify=y to ensure that the train and test sets have approximately the same proportion of churn instances\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Check the shapes of the train and test sets\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")\n",
    "\n",
    "# Check the distribution of the target variable in train and test sets to ensure that stratify=y worked\n",
    "print(f\"Churn rate in training set: {y_train.mean() * 100:.2f}%\")\n",
    "print(f\"Churn rate in test set: {y_test.mean() * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 3: Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For logistic regression, a suitable hyperparameter is the lambda parameter for regularization to prevent overfitting\n",
    "\n",
    "# Use of a simple grid search, namely ElasticNetCV, to find the optimal lambda parameter\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Create a stratified cross-validation object to split the data in a way\n",
    "# that preserves the percentage of samples for each class\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Alphas and l1_ratio parameters to search\n",
    "# alpha is the lambda parameter for regularization controlling the strength of the penalty for including more features in the model\n",
    "# l1_ratio is the ratio of L1 regularization to L2 regularization\n",
    "alphas = [0.0001, 0.001, .01, .1, 1, 10, 100]\n",
    "l1_ratios = [0.0001, .001, .01, .1, .2, .4, .8, .99, .999, 1]\n",
    "\n",
    "# Create ElasticNetCV object with stratified cross-validation\n",
    "elastic_net = ElasticNetCV(\n",
    "    l1_ratio=l1_ratios,\n",
    "    alphas=alphas,\n",
    "    cv=cv,\n",
    "    random_state=42,\n",
    "    selection='random'  # For faster computation\n",
    ")\n",
    "# Fit the model to the training data\n",
    "elastic_net.fit(X_train, y_train)\n",
    "\n",
    "# Print the optimal l1_ratio parameter\n",
    "print(f\"Optimal l1_ratio parameter: {elastic_net.l1_ratio_}\")\n",
    "\n",
    "# if logic for the optimal l1_ratio parameter, if lower than 0.5, more L2 regularization, if higher than 0.5, more L1 regularization\n",
    "if elastic_net.l1_ratio_ < 0.5:\n",
    "    print(f\"A l1_ratio of {elastic_net.l1_ratio_} means that the optimal model uses more L2 regularization than L1 regularization, so it tries to set some coefficients to zero.\")\n",
    "else:\n",
    "    print(f\"\"\"A l1_ratio of {elastic_net.l1_ratio_} means that the optimal model uses more L1 regularization than L2 regularization, \n",
    "          so it tries to reduce the coefficients as much as possible but does not set them to zero.\"\"\")\n",
    "\n",
    "# Print the optimal lambda parameter\n",
    "print(f\"\\nOptimal lambda parameter: {elastic_net.alpha_}\")\n",
    "\n",
    "# if logic for the optimal lambda parameter, if lower than 0.1, low regularization, if between 0.1 and 1, moderate regularization, if higher than 1, high regularization\n",
    "if elastic_net.alpha_ < 0.1:\n",
    "    print(f\"A lambda parameter of {elastic_net.alpha_} means that the optimal model only penalizes the coefficients slightly, so it does not use much regularization.\")\n",
    "elif 0.1 <= elastic_net.alpha_ <= 1:\n",
    "    print(f\"A lambda parameter of {elastic_net.alpha_} means that the optimal model uses moderate regularization.\")\n",
    "else:\n",
    "    print(f\"A lambda parameter of {elastic_net.alpha_} means that the optimal model uses high regularization.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The low lambda parameter makes sense as we have already removed highly correlated variables and have a small number of features.\n",
    "\n",
    "One further parameter to change is the decision threshhold, however, this is not a hyperparameter in the strictest  sense as it does not impact the model's training process. We will use the default decision threshold of 0.5 and try different thresholds later on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 4: Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the logistic regression model on the test-split of the data\n",
    "log_reg = LogisticRegression(penalty='elasticnet', l1_ratio=elastic_net.l1_ratio_, C=1/elastic_net.alpha_, solver='saga', random_state=42)\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions on the test set\n",
    "y_pred = log_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 5: Evaluating the base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score, fbeta_score, precision_score\n",
    "\n",
    "# Calculate the F-2 score of the model on the test set\n",
    "f2_score = fbeta_score(y_test, y_pred, beta=2)\n",
    "print(f\"F-2 score of the logistic regression model: {f2_score:.4f}\")\n",
    "\n",
    "# Calculate the accuracy of the model on the test set\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy of the logistic regression model: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Also calculate the recall and precision of the model on the test set\n",
    "recall = recall_score(y_test, y_pred)\n",
    "print(f\"Recall of the logistic regression model: {recall * 100:.2f}%\")\n",
    "precision = precision_score(y_test, y_pred)\n",
    "print(f\"Precision of the logistic regression model: {precision * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With an F-2 score of 0.0284, the model performs very poorly. It is interesting to see that the high accuracy of the model does not at all translate into a high F-2 score, likely because the model is very poor at predicting churners correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show some examples of correct predictions including the actual and predicted churn values\n",
    "# Create a DataFrame with test data and add columns for actual and predicted values plus their percentage certainties\n",
    "correct_pred_df = X_test.copy()\n",
    "\n",
    "correct_pred_df['Actual_Churn'] = y_test\n",
    "correct_pred_df['Predicted_Churn'] = y_pred\n",
    "\n",
    "# Add probability predictions (certainty percentages) using predict_proba which returns the probability of each class\n",
    "# In the case of a 2-class problem, it will use the sigmoid function to calculate the probability of the positive class\n",
    "y_pred_proba = log_reg.predict_proba(X_test)\n",
    "correct_pred_df['Probability_Positive_Class'] = [prob[1] * 100 for prob in y_pred_proba]\n",
    "\n",
    "# Filter for correct predictions\n",
    "correct_pred_df = correct_pred_df[correct_pred_df['Actual_Churn'] == correct_pred_df['Predicted_Churn']]\n",
    "\n",
    "# Print nr of correct predictions\n",
    "print(f\"Number of correct predictions: {correct_pred_df.shape[0]}\")\n",
    "print(f\"Percentage of correct predictions: {correct_pred_df.shape[0] / len(y_test) * 100:.2f}%\")\n",
    "\n",
    "# Display a few examples of correct predictions\n",
    "print(\"\\nExamples of correct predictions:\")\n",
    "correct_pred_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show some examples of incorrect predictions including the actual and predicted churn values\n",
    "# Create a DataFrame with test data and add columns for actual and predicted values\n",
    "incorrect_pred_df = X_test.copy()\n",
    "incorrect_pred_df['Actual_Churn'] = y_test\n",
    "incorrect_pred_df['Predicted_Churn'] = y_pred\n",
    "\n",
    "# Add probability predictions (certainty percentages) using predict_proba which returns the probability of each class\n",
    "# In the case of a 2-class problem, it will use the sigmoid function to calculate the probability of the positive class\n",
    "y_pred_proba = log_reg.predict_proba(X_test)\n",
    "incorrect_pred_df['Probability_Positive_Class'] = [prob[1] * 100 for prob in y_pred_proba]\n",
    "\n",
    "# Filter for incorrect predictions\n",
    "incorrect_pred_df = incorrect_pred_df[incorrect_pred_df['Actual_Churn'] != incorrect_pred_df['Predicted_Churn']]\n",
    "# Print nr of incorrect predictions\n",
    "print(f\"Number of incorrect predictions: {incorrect_pred_df.shape[0]}\")\n",
    "print(f\"Percentage of incorrect predictions: {incorrect_pred_df.shape[0] / len(y_test) * 100:.2f}%\")\n",
    "\n",
    "# Display a few examples of incorrect predictions\n",
    "print(\"\\nExamples of incorrect predictions:\")\n",
    "incorrect_pred_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display confusion matrix for the logistic regression model\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Create a DataFrame to display the confusion matrix\n",
    "conf_matrix_df = pd.DataFrame(conf_matrix, index=['Actual Not Churned', 'Actual Churned'], columns=['Predicted Not Churned', 'Predicted Churned'])\n",
    "\n",
    "# Display the confusion matrix\n",
    "conf_matrix_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem with the model: it predicts almost all customers not to churn and therefore there is no good indication it can even predict a customer to churn. This makes this model pretty useless for our business case. \n",
    "\n",
    "We can find a better decision threshold by examining the ROC curve to balance recall and false positive rate, then choosing a threshold that produces a better F-2 score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 6: Choosing the right decision threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ROC curve for the logistic regression model\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "# Calculate the FPR, TPR, and thresholds for the ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba[:, 1])\n",
    "\n",
    "# Calculate the AUC score\n",
    "auc_score = roc_auc_score(y_test, y_pred_proba[:, 1])\n",
    "\n",
    "# Plot the ROC curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(fpr, tpr, color='blue', label=f'Logistic Regression (AUC = {auc_score:.3f})')\n",
    "plt.plot([0, 1], [0, 1], color='red', linestyle='--', label='Random Guessing')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve - Logistic Regression')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "# Print additional information about the ROC curve\n",
    "print(f\"AUC Score: {auc_score:.3f}\")\n",
    "# Print the number of threshold points (equal to the number of unique probabilities of probability outputs of the model)\n",
    "print(f\"Number of possible threshold points: {len(thresholds)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering that an AUC score of 0.5 is equivalent to random guessing in a classification task, the AUC of 0.661 is quite poor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract TP, TN, FP, and FN from the confusion matrix\n",
    "_thresholdTP = conf_matrix_df.loc['Actual Churned', 'Predicted Churned']\n",
    "TN = conf_matrix_df.loc['Actual Not Churned', 'Predicted Not Churned']\n",
    "FP = conf_matrix_df.loc['Actual Not Churned', 'Predicted Churned']\n",
    "FN = conf_matrix_df.loc['Actual Churned', 'Predicted Not Churned']\n",
    "\n",
    "# Create a new dataframe to store threshold and F-2 score\n",
    "f2_scores = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    # Predict using the current threshold\n",
    "    y_pred_threshold = [1 if prob[1] >= threshold else 0 for prob in y_pred_proba]\n",
    "    \n",
    "    # Calculate TP, TN, FP, FN for this threshold\n",
    "    TP_threshold = sum((a == 1) and (p == 1) for a, p in zip(y_test, y_pred_threshold))\n",
    "    TN_threshold = sum((a == 0) and (p == 0) for a, p in zip(y_test, y_pred_threshold))\n",
    "    FP_threshold = sum((a == 0) and (p == 1) for a, p in zip(y_test, y_pred_threshold))\n",
    "    FN_threshold = sum((a == 1) and (p == 0) for a, p in zip(y_test, y_pred_threshold))\n",
    "    \n",
    "    # Calculate precision and recall\n",
    "    precisions = TP_threshold / (TP_threshold + FP_threshold) if (TP_threshold + FP_threshold) > 0 else 0\n",
    "    recalls = TP_threshold / (TP_threshold + FN_threshold) if (TP_threshold + FN_threshold) > 0 else 0\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracies = (TP_threshold + TN_threshold) / (TP_threshold + TN_threshold + FP_threshold + FN_threshold)\n",
    "    \n",
    "    # Calculate F-2 score: (5 * precision * recall) / (4 * precision + recall)\n",
    "    f2 = (5 * precisions * recalls) / ((4 * precisions) + recalls) if ((4 * precisions) + recalls) > 0 else 0\n",
    "    f2_scores.append(f2)\n",
    "\n",
    "f2_df = pd.DataFrame({\n",
    "    'Threshold': thresholds,\n",
    "    'F2_Score': f2_scores,\n",
    "    'Precision': precisions,\n",
    "    'Recall': recalls,\n",
    "    'Accuracy': accuracies\n",
    "})\n",
    "\n",
    "top_5_thresholds = f2_df.nlargest(5, 'F2_Score')\n",
    "# Display the top 5 thresholds with highest F-2 score\n",
    "top_5_thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the best threshold (highest accuracy)\n",
    "best_threshold = top_5_thresholds.iloc[0]['Threshold']\n",
    "best_f2 = top_5_thresholds.iloc[0]['F2_Score']\n",
    "print(f\"The best threshold is {best_threshold} with an F-2 score of {best_f2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 7: Evaluate final Logistic Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show some examples of correct predictions including the actual and predicted churn values with a threshold of 0.0896728999861171 \n",
    "# Create a DataFrame with test data and add columns for actual and predicted values plus their percentage certainties\n",
    "correct_pred_df_final = X_test.copy()\n",
    "\n",
    "correct_pred_df_final['Actual_Churn'] = y_test\n",
    "\n",
    "# Add probability predictions (certainty percentages) using predict_proba which returns the probability of each class\n",
    "# In the case of a 2-class problem, it will use the sigmoid function to calculate the probability of the positive class\n",
    "y_pred_proba = log_reg.predict_proba(X_test)\n",
    "correct_pred_df_final['Probability_Positive_Class'] = [prob[1] * 100 for prob in y_pred_proba]\n",
    "\n",
    "# Set the threshold for the certainty percentage to 0.0896728999861171 \n",
    "threshold = best_threshold\n",
    "\n",
    "# Add column to show predicted churn based on the threshold\n",
    "correct_pred_df_final['Predicted_Churn'] = [1 if prob/100 >= threshold else 0 for prob in correct_pred_df_final['Probability_Positive_Class']]\n",
    "\n",
    "# Filter for correct predictions\n",
    "correct_pred_df_final = correct_pred_df_final[correct_pred_df_final['Actual_Churn'] == correct_pred_df_final['Predicted_Churn']]\n",
    "\n",
    "\n",
    "# Print nr of correct predictions\n",
    "print(f\"Number of correct predictions with threshold of {threshold}: {correct_pred_df_final.shape[0]}\")\n",
    "print(f\"Percentage of correct predictions with threshold of {threshold}: {correct_pred_df_final.shape[0] / len(y_test) * 100:.2f}%\")\n",
    "\n",
    "# Display a few examples of correct predictions\n",
    "print(f\"\\nExamples of correct predictions with threshold of {best_threshold}:\")\n",
    "correct_pred_df_final.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show some examples of incorrect predictions including the actual and predicted churn values with a threshold of 0.135914\n",
    "# Create a DataFrame with test data and add columns for actual and predicted values\n",
    "incorrect_pred_df_final = X_test.copy()\n",
    "\n",
    "incorrect_pred_df_final['Actual_Churn'] = y_test\n",
    "\n",
    "# Follow steps above\n",
    "incorrect_pred_df_final['Probability_Positive_Class'] = [prob[1] * 100 for prob in y_pred_proba]\n",
    "\n",
    "# Add column to show predicted churn based on the threshold\n",
    "incorrect_pred_df_final['Predicted_Churn'] = [1 if prob/100 >= threshold else 0 for prob in incorrect_pred_df_final['Probability_Positive_Class']]\n",
    "\n",
    "# Filter for incorrect predictions\n",
    "incorrect_pred_df_final = incorrect_pred_df_final[incorrect_pred_df_final['Actual_Churn'] != incorrect_pred_df_final['Predicted_Churn']]\n",
    "\n",
    "# Print nr of incorrect predictions\n",
    "print(f\"Number of incorrect predictions with threshold of {threshold}: {incorrect_pred_df_final.shape[0]}\")\n",
    "print(f\"Percentage of incorrect predictions with threshold of {threshold}: {incorrect_pred_df_final.shape[0] / len(y_test) * 100:.2f}%\")\n",
    "\n",
    "# Display a few examples of incorrect predictions\n",
    "print(\"\\nExamples of incorrect predictions with threshold of 0.135914:\")\n",
    "incorrect_pred_df_final.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fuse together the correct and incorrect predictions DataFrames with the threshold at 0.135914 to display the confusion matrix\n",
    "final_pred_df = pd.concat([correct_pred_df_final, incorrect_pred_df_final])\n",
    "\n",
    "# Print shape of final_pred_df\n",
    "print(f\"Shape of final_pred_df: {final_pred_df.shape}\")\n",
    "final_pred_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and display confusion matrix of the final predictions\n",
    "final_conf_matrix = confusion_matrix(final_pred_df['Actual_Churn'], final_pred_df['Predicted_Churn'])\n",
    "\n",
    "# Create a DataFrame to display the confusion matrix\n",
    "final_conf_matrix_df = pd.DataFrame(final_conf_matrix, index=['Actual Not Churned', 'Actual Churned'], columns=['Predicted Not Churned', 'Predicted Churned'])\n",
    "\n",
    "# Display the confusion matrix\n",
    "final_conf_matrix_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "# Show confusion matrix of base model and final model next to each other\n",
    "plt.figure(figsize=(16, 6))\n",
    "conf_matrix_df\n",
    "# Create a figure with two subplots side by side\n",
    "fig, axs = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot confusion matrix of base model (left)\n",
    "sns.heatmap(conf_matrix_df, annot=True, fmt='d', cmap='Blues', ax=axs[0])\n",
    "axs[0].set_title('Base Model Confusion Matrix')\n",
    "axs[0].set_ylabel('Actual')\n",
    "axs[0].set_xlabel('Predicted')\n",
    "\n",
    "# Plot confusion matrix of final model (right)\n",
    "sns.heatmap(final_conf_matrix_df, annot=True, fmt='d', cmap='Blues', ax=axs[1])\n",
    "axs[1].set_title('Final Model with Optimized Threshold')\n",
    "axs[1].set_ylabel('Actual')\n",
    "axs[1].set_xlabel('Predicted')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate F-2 score, precision, recall and accuracy of the final model\n",
    "final_f2_score = fbeta_score(final_pred_df['Actual_Churn'], final_pred_df['Predicted_Churn'], beta=2)\n",
    "final_precision = precision_score(final_pred_df['Actual_Churn'], final_pred_df['Predicted_Churn'])\n",
    "final_recall = recall_score(final_pred_df['Actual_Churn'], final_pred_df['Predicted_Churn'])\n",
    "final_accuracy = accuracy_score(final_pred_df['Actual_Churn'], final_pred_df['Predicted_Churn'])\n",
    "\n",
    "# Print the F-2 score, precision, recall, and accuracy of the base model and final model\n",
    "print(f\"Base Model - F-2 Score: {f2_score:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Final Model - F-2 Score: {final_f2_score:.4f}, Precision: {final_precision:.4f}, Recall: {final_recall:.4f}, Accuracy: {final_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final model has a bit higher accuracy, however, it is even worse at predicting churn correctly. Therefore, we will now use a random forest with XGBoost in the hopes of creating a better model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 2: XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is chosen because while it is not as interpretable as a logistic regression, XGBoost is highly relevant for customer churn prediction because of its ability to handle structured data, detect complex nonlinear patterns, and deal with imbalanced datasets. Additionally, it has regularization capabilities and the importance of each feature can be determined to get an insight into why the model performs the way it does."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 1: Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this model, we leave in all features, including the state, to give the model the best possible chance of predicting churn well.\n",
    "# Transform state column into dummy variables\n",
    "model2_df = pd.get_dummies(df, columns=['State'], drop_first=True)\n",
    "\n",
    "# Create binary variables for categorical columns (International plan, voice mail plan, and state)\n",
    "model2_df['International plan'] = model2_df['International plan'].map({'Yes': 1, 'No': 0})\n",
    "model2_df['Voice mail plan'] = model2_df['Voice mail plan'].map({'Yes': 1, 'No': 0})\n",
    "model2_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will also not drop the columns that are perfectly linearly correlated because a random forest can better handle multicollinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also, again change Churn column from boolean to int with Churn = False as 0 and Churn = True as 1\n",
    "model2_df['Churn'] = model2_df['Churn'].astype(int)\n",
    "model2_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, for this model we will add additional features\n",
    "# Add total charge column\n",
    "model2_df['Total charge'] = model2_df['Total day charge'] + model2_df['Total eve charge'] + model2_df['Total night charge'] + model2_df['Total intl charge']\n",
    "\n",
    "# Add total minutes column\n",
    "model2_df['Total minutes'] = model2_df['Total day minutes'] + model2_df['Total eve minutes'] + model2_df['Total night minutes'] + model2_df['Total intl minutes']\n",
    "\n",
    "# Add total calls column\n",
    "model2_df['Total calls'] = model2_df['Total day calls'] + model2_df['Total eve calls'] + model2_df['Total night calls'] + model2_df['Total intl calls']\n",
    "\n",
    "# Add average call duration\n",
    "model2_df['Average call duration'] = model2_df['Total minutes'] / model2_df['Total calls']\n",
    "\n",
    "# Add Average Charge per Call\n",
    "model2_df['Average charge per call'] = model2_df['Total charge'] / model2_df['Total calls']\n",
    "model2_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2: Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use same train-test split as before\n",
    "X_2 = model2_df.drop(['Churn'], axis=1)  # Features\n",
    "y_2 = model2_df['Churn']  # Target variable\n",
    "\n",
    "X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(X_2, y_2, test_size=0.2, random_state=41, stratify=y_2)\n",
    "\n",
    "# Check the shapes of the train and test sets\n",
    "print(f\"X_train_2 shape: {X_train_2.shape}\")\n",
    "print(f\"X_test_2 shape: {X_test_2.shape}\")\n",
    "print(f\"y_train_2 shape: {y_train_2.shape}\")\n",
    "print(f\"y_test_2 shape: {y_test_2.shape}\")\n",
    "\n",
    "# Check the distribution of the target variable in train and test sets to ensure that stratify=y worked\n",
    "print(f\"Churn rate in training set: {y_train.mean() * 100:.2f}%\")\n",
    "print(f\"Churn rate in test set: {y_test.mean() * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 3: Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Create a pipeline that applies SMOTE bc of class imbalance and then trains a decision tree classifier\n",
    "# The pipeline will be used to fit the model to the training data\n",
    "pipeline = Pipeline([\n",
    "    ('smote', SMOTE(random_state=42)),\n",
    "    ('xgb', XGBClassifier(random_state=42))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the hyperparameter grid for the decision tree classifier\n",
    "\n",
    "param_grid = {\n",
    "    'xgb__base_score': [0.5],         # Initial prediction score for all instances, lower values predict less churn\n",
    "    'xgb__booster': ['gbtree'],  # Algorithm to use - tree-based, linear model\n",
    "    'xgb__colsample_bylevel': [1.0],        # Subsample ratio of columns for each level within a tree\n",
    "    'xgb__colsample_bytree': [1.0],         # Subsample ratio of columns when constructing each tree\n",
    "    'xgb__gamma': [0, 0.1],            # Minimum loss reduction required for further partition on a leaf node\n",
    "    'xgb__n_estimators': [75, 100],   # Number of trees to build (more trees = better performance but slower)\n",
    "    'xgb__max_depth': [5, 10],    # Maximum depth of a tree (controls complexity)\n",
    "    'xgb__learning_rate': [0.9, 0.1],   # Step size shrinkage to prevent overfitting\n",
    "    'xgb__min_child_weight': [1, 2],        # Minimum sum of instance weight needed in a child\n",
    "    'xgb__objective': ['binary:logistic'],   # Defines the loss function (binary classification with logistic)\n",
    "    'xgb__random_state': [42],              # Random number seed for reproducibility\n",
    "    'xgb__reg_alpha': [0, 0.1],        # L1 regularization on weights (prevents overfitting)\n",
    "    'xgb__reg_lambda': [1, 2],           # L2 regularization on weights (prevents overfitting)\n",
    "    'xgb__subsample': [1]                   # Subsample ratio of training instances (1 = use all samples)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Set up stratified 5-fold cross-validation\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Set up the grid search with the pipeline, parameter grid, and cross-validation\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid,\n",
    "    cv=cv,\n",
    "    scoring='accuracy',  # Use accuracy as the scoring metric\n",
    "    n_jobs=1,  # Use one CPU core\n",
    "    verbose=2,  # Provide detailed output\n",
    ")\n",
    "\n",
    "# Fit the grid search to the training data\n",
    "grid_search.fit(X_train_2, y_train_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output best parameters and cross-validation accuracy\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "print(\"Best CV Accuracy: {:.2f}%\".format(grid_search.best_score_*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 4: Evaluating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on the test set using the best found model\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred_2 = best_model.predict(X_test_2)\n",
    "\n",
    "# Calculate the accuracy of the model on the test set\n",
    "accuracy_2 = accuracy_score(y_test_2, y_pred_2)\n",
    "print(f\"Accuracy of the XGBoost model: {accuracy_2 * 100:.2f}%\")\n",
    "\n",
    "# Also calculate the recall of the model on the test set because when predicting churn, we want to minimize false negatives\n",
    "recall_2 = recall_score(y_test_2, y_pred_2)\n",
    "print(f\"Recall of the XGBoost model: {recall_2 * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show some examples of correct predictions including the actual and predicted churn values\n",
    "# Create a DataFrame with test data and add columns for actual and predicted values plus their percentage certainties\n",
    "correct_pred_df_2 = X_test_2.copy()\n",
    "\n",
    "correct_pred_df_2['Actual_Churn'] = y_test_2\n",
    "correct_pred_df_2['Predicted_Churn'] = y_pred_2\n",
    "\n",
    "# Filter for correct predictions\n",
    "correct_pred_df_2 = correct_pred_df_2[correct_pred_df_2['Actual_Churn'] == correct_pred_df_2['Predicted_Churn']]\n",
    "\n",
    "# Print nr of correct predictions\n",
    "print(f\"Number of correct predictions: {correct_pred_df_2.shape[0]}\")\n",
    "print(f\"Percentage of correct predictions: {correct_pred_df_2.shape[0] / len(y_test_2) * 100:.2f}%\")\n",
    "\n",
    "# Display a few examples of correct predictions\n",
    "print(\"\\nExamples of correct predictions:\")\n",
    "correct_pred_df_2.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show some examples of incorrect predictions including the actual and predicted churn values\n",
    "# Create a DataFrame with test data and add columns for actual and predicted values\n",
    "incorrect_pred_df_2 = X_test_2.copy()\n",
    "incorrect_pred_df_2['Actual_Churn'] = y_test_2\n",
    "incorrect_pred_df_2['Predicted_Churn'] = y_pred_2\n",
    "\n",
    "# Filter for incorrect predictions\n",
    "incorrect_pred_df_2 = incorrect_pred_df_2[incorrect_pred_df_2['Actual_Churn'] != incorrect_pred_df_2['Predicted_Churn']]\n",
    "# Print nr of incorrect predictions\n",
    "print(f\"Number of incorrect predictions: {incorrect_pred_df_2.shape[0]}\")\n",
    "print(f\"Percentage of incorrect predictions: {incorrect_pred_df_2.shape[0] / len(y_test_2) * 100:.2f}%\")\n",
    "\n",
    "# Display a few examples of incorrect predictions\n",
    "print(\"\\nExamples of incorrect predictions:\")\n",
    "incorrect_pred_df_2.head(18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display confusion matrix for the XGBoost Random Forest Model\n",
    "# Calculate the confusion matrix\n",
    "conf_matrix_2 = confusion_matrix(y_test_2, y_pred_2)\n",
    "\n",
    "# Create a DataFrame to display the confusion matrix\n",
    "conf_matrix_df_2 = pd.DataFrame(conf_matrix_2, index=['Actual Not Churned', 'Actual Churned'], columns=['Predicted Not Churned', 'Predicted Churned'])\n",
    "\n",
    "# Display the confusion matrix\n",
    "conf_matrix_df_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 5: Comparing both models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing the confusion matrices of the logistic regression model and the XGBoost Random Forest model\n",
    "plt.figure(figsize=(16, 6))\n",
    "conf_matrix_df\n",
    "# Create a figure with two subplots side by side\n",
    "fig, axs = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot confusion matrix of logistic regression model (left)\n",
    "sns.heatmap(conf_matrix_df, annot=True, fmt='d', cmap='Blues', ax=axs[0])\n",
    "axs[0].set_title('Logistic Regression Model Confusion Matrix')\n",
    "axs[0].set_ylabel('Actual')\n",
    "axs[0].set_xlabel('Predicted')\n",
    "\n",
    "# Plot confusion matrix of XGBoost Random Forest model (right)\n",
    "sns.heatmap(conf_matrix_df_2, annot=True, fmt='d', cmap='Blues', ax=axs[1])\n",
    "axs[1].set_title('XGBoost Random Forest Model Confusion Matrix')\n",
    "axs[1].set_ylabel('Actual')\n",
    "axs[1].set_xlabel('Predicted')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print precision, recall and accuracy of both models\n",
    "print(f\"Logistic Regression Model - Recall: {recall:.2f}, Accuracy: {accuracy:.2f}\")\n",
    "print(f\"XGBoost Random Forest Model - Recall: {recall_2:.2f}, Accuracy: {accuracy_2:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing the ROC curves of the logistic regression model and the XGBoost Random Forest model\n",
    "# Calculate the FPR, TPR, and thresholds for the ROC curve\n",
    "fpr_2, tpr_2, thresholds_2 = roc_curve(y_test_2, best_model.predict_proba(X_test_2)[:, 1])\n",
    "\n",
    "# Calculate the AUC score\n",
    "auc_score_2 = roc_auc_score(y_test_2, best_model.predict_proba(X_test_2)[:, 1])\n",
    "\n",
    "# Plot the ROC curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(fpr, tpr, color='blue', label=f'Logistic Regression (AUC = {auc_score:.3f})')\n",
    "plt.plot(fpr_2, tpr_2, color='green', label=f'XGBoost Random Forest (AUC = {auc_score_2:.3f})')\n",
    "plt.plot([0, 1], [0, 1], color='red', linestyle='--', label='Random Guessing')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve - Logistic Regression vs XGBoost Random Forest')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The XGBoost model is much better at predicting churned customers than the logistic regression and only marginally worse at predicting non-churners. Therefore, it makes sense to use this model going forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 6: Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at plot of feature importance\n",
    "import xgboost as xgb\n",
    "\n",
    "# Access the XGBClassifier from the pipeline\n",
    "xgb_model_classifier = best_model.named_steps['xgb']\n",
    "\n",
    "# Plot feature importances using xgboost's built-in function\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "# Gain is used to measure the improvement in accuracy brought by a feature to the branches it is on because \n",
    "# accuracy is the main goal of the model\n",
    "xgb.plot_importance(xgb_model_classifier, importance_type=\"gain\", ax=ax, max_num_features=20)\n",
    "plt.title('Feature Importance from XGBoost Model')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3.1 Which performance metric did you use to evaluate the performance? Why? [Free text + code]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I selected accuracy as the performance metric because that is the metric the professor said the models will have their performance graded on. Otherwise, I would have chosen the F-2 score. The F-2 score merges recall and precision into a single score, providing a good blend between how well the model can predict churners well and if it overpredicts non-churners to be churners. Unlike the F-1 score, the F-2 score puts more emphasis on recall than on precision. This makes sense because the telcom company should be more concerned about correctly predicting all possible churning customers (recall) than predicting too many non-churning customers to be churners (precision) because a churned customer likely costs the company a lot more than being overcareful with a non-churning customer. On the other hand, just focusing entirely on recall will incentivize the model just to predict all customers as churners which is not ideal because it will involve a lot of unnecessary effort on the part of the telcom company to keep customers happy that are not planning on leaving in the first place. Therefore, the F-2 score provides a very good balance and makes most sense to me as a performance metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3.2 Which model provided the best results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown previously, the XGBoost model provided the best results both in terms of accuracy and recall when compared to the logistic regression. The ROC curves of both models show that this pattern holds at almost all threshold levels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4: The executive team has provided an external validation dataset with the same structure as the original data. To test the best model, the team must ensure it works seamlessly on new data. The model should predict churn using binary outputs: 0 (No Churn) and 1 (Churn).\n",
    "\n",
    "- Save the best-performing model as a Pickle file (StudentNumber_Pipeline.pkl).\n",
    "Please see the code Assignment1_SaveAsPickle.ipynb with the examples\n",
    "- To ensure the code runs in every machine please save the requirements files with the\n",
    "name StudentNumber_requirements.txt  please see\n",
    "Assignment1_SaveAsPickle.ipynb on how to do it.\n",
    "    - For this to happen you will also require submitting a .py file with your feature\n",
    "engineer function, please keep the following name feature_engineering.py\n",
    "- If you are using a model outside the scikit-learn you will need to save the following\n",
    "files:\n",
    "    - StudentNumber_Preprocessor.pkl  where the preprocessing steps are\n",
    "performed\n",
    "    - StudentNumber_Model.pkl - where the model does the prediction task.\n",
    "- Write code to reload the saved model and test it on the external validation dataset\n",
    "(please see Assignment1_SaveAsPickle.ipynb on how to do it. The validation set\n",
    "will be in the format from the file\n",
    "2767ML_assignment1_externalvalidation_data_toStudents.csv). If your\n",
    "model runs in this dataset it will run in the final validation set.\n",
    "- Ensure the model outputs predictions in the required format 0 (No Churn) and 1\n",
    "(Churn).\n",
    "\n",
    "How This Question Will Be Evaluated:\n",
    "1. Top 25% Models: The models that perform the best on the external validation dataset\n",
    "will receive full marks for this task. Performance will be ranked based on accuracy.\n",
    "2. Next 25% Models: Students whose models perform in the following 25% will have 3\n",
    "and so on.\n",
    "3. Non-Functional Models: If your model fails to load, run, or provide predictions in the\n",
    "required format (0 for No Churn, 1 for Churn), you will receive zero marks for this\n",
    "task. Make sure your .pkl file and code are functional, tested, and well-documented.\n",
    "    1. You can find a test set in the assignment on moodle with the same format\n",
    "as the final (2767ML_assignment1_externalvalidation_data_toStudents.csv)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load initial dataset\n",
    "df = pd.read_csv('2767ML_assignment1_data.csv')\n",
    "\n",
    "# Identify numerical and categorical columns\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import pickle\n",
    "\n",
    "# Save numerical columns to a list\n",
    "numerical_columns = ['Account length', 'Area code', 'Number vmail messages', 'Total day minutes', 'Total day calls', 'Total day charge', 'Total eve minutes', 'Total eve calls', 'Total eve charge', 'Total night minutes', 'Total night calls', 'Total night charge', 'Total intl minutes', 'Total intl calls', 'Total intl charge', 'Customer service calls']\n",
    "\n",
    "# Save categorical columns to a list\n",
    "categorical_columns = ['State', 'International plan', 'Voice mail plan']\n",
    "\n",
    "# Convert Churn from boolean to integer (0 for False, 1 for True)\n",
    "df['Churn'] = df['Churn'].astype(int)\n",
    "\n",
    "# Split the data into features and target variable\n",
    "X_save = df.drop('Churn', axis=1)\n",
    "Y_save = df['Churn']\n",
    "\n",
    "# Define function to perform feature selection steps\n",
    "def feature_engineering(X_save):\n",
    "    # Add total charge column\n",
    "    X_save['Total charge'] = X_save['Total day charge'] + X_save['Total eve charge'] + X_save['Total night charge'] + X_save['Total intl charge']\n",
    "    # Add total minutes column\n",
    "    X_save['Total minutes'] = X_save['Total day minutes'] + X_save['Total eve minutes'] + X_save['Total night minutes'] + X_save['Total intl minutes']\n",
    "    # Add total calls column\n",
    "    X_save['Total calls'] = X_save['Total day calls'] + X_save['Total eve calls'] + X_save['Total night calls'] + X_save['Total intl calls']\n",
    "    # Add average call duration\n",
    "    X_save['Average call duration'] = X_save['Total minutes'] / X_save['Total calls']\n",
    "    # Add Average Charge per Call\n",
    "    X_save['Average charge per call'] = X_save['Total charge'] / X_save['Total calls']\n",
    "\n",
    "    return X_save\n",
    "\n",
    "# Apply FunctionTransformer to apply the feature engineering function\n",
    "feature_transformer = FunctionTransformer(feature_engineering)\n",
    "\n",
    "# Handle NaN values in the dataset\n",
    "numerical_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),  # Replace NaN with median\n",
    "    ('scaler', StandardScaler())  # Scale the numerical data\n",
    "])\n",
    "\n",
    "# Handle categorical values in the dataset\n",
    "categorical_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value=np.nan)),  # Replace NaN with 'missing'\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))  # One-hot encode the categorical data\n",
    "])\n",
    "\n",
    "# 3 Combine both transformations into a ColumnTransformer\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', numerical_transformer, numerical_columns),\n",
    "    ('cat', categorical_transformer, categorical_columns)\n",
    "])\n",
    "\n",
    "# Create the full pipeline with preprocessing + feature engineering + model\n",
    "preprocessing_pipeline = Pipeline([\n",
    "    ('feature_engineering', feature_transformer), \n",
    "    ('preprocessor', preprocessor),\n",
    "])\n",
    "\n",
    "# Apply preprocessing transformations\n",
    "X_transformed = preprocessing_pipeline.fit_transform(X_save)\n",
    "\n",
    "# Train-test split\n",
    "X_train_save, X_test_save, Y_train_save, Y_test_save = train_test_split(X_transformed, Y_save, test_size=0.20, random_state=42)\n",
    "\n",
    "# Train **XGBoost** Model\n",
    "xgb_model = best_model\n",
    "xgb_model.fit(X_train_save, Y_train_save)\n",
    "\n",
    "# Save **Preprocessing + Feature Engineering Together**\n",
    "with open(\"preprocessor.pkl\", \"wb\") as file:\n",
    "    pickle.dump(preprocessing_pipeline, file)\n",
    "\n",
    "# Save **XGBoost Model** as Pickle\n",
    "with open(\"Finn_Hetzler_Model.pkl\", \"wb\") as file:\n",
    "    pickle.dump(xgb_model, file)  # Saving XGBClassifier using Pickle\n",
    "\n",
    "print(\"Preprocessing pipeline (feature engineering + transformations) and XGBoost model saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Install the requirements, load the model and apply it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [],
   "source": [
    "!pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_engineering import feature_engineering\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "# Load the preprocessing pipeline (including feature engineering)\n",
    "with open(\"preprocessor.pkl\", \"rb\") as file:\n",
    "    loaded_preprocessor = pickle.load(file)\n",
    "\n",
    "# Load the XGBoost model (pickled)\n",
    "with open(\"Finn_Hetzler_Model.pkl\", \"rb\") as file:\n",
    "    xgb_loaded_model = pickle.load(file)\n",
    "\n",
    "print(\"Preprocessing pipeline (feature engineering + transformations) and XGBoost model loaded successfully!\")\n",
    "\n",
    "# New data to make a prediction\n",
    "validation_df = pd.read_csv('2767ML_assignment1_externalvalidation_data_toStudents.csv')\n",
    "validation_df.head()\n",
    "\n",
    "# Apply feature engineering and preprocessing together\n",
    "new_data_transformed = loaded_preprocessor.transform(validation_df)\n",
    "\n",
    "# Make prediction using the loaded XGBoost model\n",
    "xgb_prediction = xgb_loaded_model.predict(new_data_transformed)\n",
    "\n",
    "print(f\"XGBoost Prediction: {xgb_prediction[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5: The executive team requires actionable insights to guide the strategy to address customer churn effectively. Your analysis will directly inform their decisions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5.1 What customer characteristics most strongly influence churn? [Free text]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most important features were already shown earlier in the plot in Step 6 of Model 2: [Link](#step-6-feature-importance). Note, I wanted to put the plot down here but for some reason the feature names were not showing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this plot, the most important features are the Total Charge, Customer service calls and Number vmail messages of the customer. It is particularly noteworthy that Total Charge was the most important feature, considering all of the features that were summed up to make this feature (Total day charge, Total eve charge, Total night charge, Total intl charge) were left in the model and therefore likely take away feature importance from Total Charge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5.2 What actionable steps should the company take to reduce churn? Suggest two strategies. [Free text]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The company should collect feedback from customers the model predicts are likely to churn. By proactively asking these people, for example with a survey, they can address the issues they have with the current service, so the company knows what to change so that they remain customers.\n",
    "\n",
    "2. As seen above in the 15 most important features, one of the states, namely \"WV\" or West Virginia is also an important feature. It could also make sense to take a closer look at what customers are unsatisfied with there. For example, if they say network quality is not good, then they can look into improving their network infrastructure. Alternatively, if there is a cheaper competitor, they can look into lowering prices to remain competitive."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
